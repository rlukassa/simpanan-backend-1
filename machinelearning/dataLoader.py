"""
Enhanced Data Loader - Uses processed high-quality CSV data
Auto-generated by chatbot.ipynb processing pipeline
"""
import pandas as pd
import os
import sys

# Add current directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

def load_csv_data():
    """Load processed high-quality CSV data for chatbot (ENHANCED VERSION)"""
    # Try to load processed data first
    processed_file = os.path.join(
        os.path.dirname(__file__), 
        'database', 
        'processed', 
        'itb_chatbot_high_quality_20250621_190153.csv'
    )

    if os.path.exists(processed_file):
        try:
            print(f"ðŸ“‚ Loading enhanced dataset: {os.path.basename(processed_file)}")
            df = pd.read_csv(processed_file)

            all_data = []
            for _, row in df.iterrows():
                entry = {
                    'source': row['data_source'],
                    'content': row['content'],
                    'processed_content': row['content_cleaned'],
                    'category': row['category'],
                    'quality_score': row['quality_score'],
                    'content_length': row['content_length'],
                    'type': row.get('type', ''),
                    'links': row.get('links', ''),
                    'record_id': row['record_id']
                }
                all_data.append(entry)

            print(f"âœ… Loaded {len(all_data)} high-quality entries")
            print(f"ðŸ“Š Categories: {len(set(entry['category'] for entry in all_data))}")
            print(f"â­ Avg quality: {sum(entry['quality_score'] for entry in all_data)/len(all_data):.1f}/100")

            return all_data

        except Exception as e:
            print(f"âš ï¸  Error loading processed data: {e}")
            print("ðŸ”„ Falling back to original CSV files...")
    else:
        print(f"âš ï¸  Processed file not found: {processed_file}")
        print("ðŸ”„ Using original CSV files...")

    # Fallback to original method
    return load_original_csv_data()

def load_original_csv_data():
    """Original CSV data loading method (fallback)"""
    data_dir = os.path.join(os.path.dirname(__file__), 'database', 'data')

    csv_files = [
        'tentangITB.csv',
        'wikipediaITB.csv', 
        'multikampusITB.csv'
    ]

    all_data = []

    for csv_file in csv_files:
        file_path = os.path.join(data_dir, csv_file)
        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
            try:
                df = pd.read_csv(file_path)
                if not df.empty and 'content' in df.columns:
                    df_filtered = df[df['content'].notna() & (df['content'] != '')]

                    for _, row in df_filtered.iterrows():
                        content = str(row['content']).strip()
                        if content and len(content) >= 5:
                            entry = {
                                'source': csv_file.replace('.csv', ''),
                                'content': content,
                                'processed_content': content.lower(),
                                'type': row.get('type', ''),
                                'links': row.get('links', ''),
                                'category': 'uncategorized',
                                'quality_score': 50,  # Default score
                                'content_length': len(content)
                            }
                            all_data.append(entry)

            except Exception as e:
                print(f"Error loading {csv_file}: {e}")
                continue

    print(f"Loaded {len(all_data)} data entries from original CSV files")
    return all_data

def get_sample_data():
    """Get sample of loaded data for testing"""
    data = load_csv_data()
    return data[:10] if data else []

if __name__ == "__main__":
    # Test the enhanced loader
    data = load_csv_data()
    print(f"\nTotal entries: {len(data)}")

    if data:
        print("\nSample entries:")
        for i, entry in enumerate(data[:3]):
            score = entry.get('quality_score', 'N/A')
            category = entry.get('category', 'uncategorized')
            print(f"{i+1}. [{entry['source']}] {category} ({score}) {entry['content'][:80]}...")
